{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Flatten, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy, and french model\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/News'# data directory containing input.txt\n",
    "save_dir = 'save' # directory to store models\n",
    "seq_length = 30 # sequence length\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\"]\n",
    "\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "for file_name in file_list:\n",
    "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
    "    #read data\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    #create sentences\n",
    "    doc = nlp(data)\n",
    "    wl = create_wordlist(doc)\n",
    "    wordlist = wordlist + wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  1842\n"
     ]
    }
   ],
   "source": [
    "# count the number of words\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "#size of the vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "#save the words and vocabulary\n",
    "with open(os.path.join(vocab_file), 'wb') as f:\n",
    "    cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 7858\n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 256 # size of RNN\n",
    "batch_size = 32 # minibatch size\n",
    "seq_length = 30 # sequence length\n",
    "num_epochs = 50 # number of epochs\n",
    "learning_rate = 0.001 #learning rate\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               4298752   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1842)              944946    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1842)              0         \n",
      "=================================================================\n",
      "Total params: 5,243,698\n",
      "Trainable params: 5,243,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7779 samples, validate on 79 samples\n",
      "Epoch 1/50\n",
      "7779/7779 [==============================] - 131s 17ms/step - loss: 6.6805 - categorical_accuracy: 0.0390 - val_loss: 6.6231 - val_categorical_accuracy: 0.0633\n",
      "Epoch 2/50\n",
      "7779/7779 [==============================] - 119s 15ms/step - loss: 6.1549 - categorical_accuracy: 0.0476 - val_loss: 6.6811 - val_categorical_accuracy: 0.0633\n",
      "Epoch 3/50\n",
      "7779/7779 [==============================] - 117s 15ms/step - loss: 6.0830 - categorical_accuracy: 0.0492 - val_loss: 6.7773 - val_categorical_accuracy: 0.0633\n",
      "Epoch 4/50\n",
      "7779/7779 [==============================] - 116s 15ms/step - loss: 5.9585 - categorical_accuracy: 0.0505 - val_loss: 6.7415 - val_categorical_accuracy: 0.0127\n",
      "Epoch 5/50\n",
      "7779/7779 [==============================] - 116s 15ms/step - loss: 5.7765 - categorical_accuracy: 0.0541 - val_loss: 6.6809 - val_categorical_accuracy: 0.0633\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences_lstm.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "history = md.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "md.save(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the model\n",
    "print(\"loading model...\")\n",
    "model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with the following seed: \"a a a a a a a a a a a a a a a a le vainqueur de 12 tournois majeurs ne veut pas imposer trop de pression .\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initiate sentences\n",
    "seed_sentences = \"le vainqueur de 12 tournois majeurs ne veut pas imposer trop de pression .\"\n",
    "generated = ''\n",
    "sentence = []\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
    "\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a a a a a le vainqueur de 12 tournois majeurs ne veut pas imposer trop de pression . de et . à , . de et de la , , de , . , et , - nous la joueurs . il , a des sera et l' carrière le , plus en se un autant , . . . » « , qui de dans , a un ( un de de de ai de présenté de les en celle la il . de ce était , de , en la 4 , , , de , à six « « , le chances , , , la de la l’ 4 . de le de grand\n"
     ]
    }
   ],
   "source": [
    "words_number = 100\n",
    "#generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "    #print(x.shape)\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.33)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "data_dir = 'data/News'# data directory containing input.txt\n",
    "save_dir = 'save' # directory to store models\n",
    "file_list = [\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import spacy, and french model\n",
    "import spacy\n",
    "nlp = spacy.load('fr')\n",
    "\n",
    "#initiate sentences and labels lists\n",
    "sentences = []\n",
    "sentences_label = []\n",
    "\n",
    "#create sentences function:\n",
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences\n",
    "\n",
    "#create sentences from files\n",
    "for file_name in file_list:\n",
    "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
    "    #read data\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    #create sentences\n",
    "    doc = nlp(data)\n",
    "    sents = create_sentences(doc)\n",
    "    sentences = sentences + sents\n",
    "    \n",
    "#create labels\n",
    "for i in range(np.array(sentences).shape[0]):\n",
    "    sentences_label.append(\"ID\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(data, docLabels, size=300, sample=0.000001, dm=0, hs=1, window=10, min_count=0, workers=8,alpha=0.024, min_alpha=0.024, epoch=15, save_file='./data/doc2vec.w2v') :\n",
    "    startime = time.time()\n",
    "    \n",
    "    print(\"{0} articles loaded for model\".format(len(data)))\n",
    "\n",
    "    it = LabeledLineSentence(data, docLabels)\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=size, sample=sample, dm=dm, window=window, min_count=min_count, workers=workers,alpha=alpha, min_alpha=min_alpha, hs=hs) # use fixed learning rate\n",
    "    model.build_vocab(it)\n",
    "    for epoch in range(epoch):\n",
    "        print(\"Training epoch {}\".format(epoch + 1))\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        # model.alpha -= 0.002 # decrease the learning rate\n",
    "        # model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "        \n",
    "    #saving the created model\n",
    "    model.save(os.path.join(save_file))\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314 articles loaded for model\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  import sys\n",
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(sentences, sentences_label, size=500,sample=0.0,alpha=0.025, min_alpha=0.001, min_count=0, window=10, epoch=20, dm=0, hs=1, save_file='./data/doc2vec.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : ['un', 'mois', 'après', 'la', 'mort', 'de', 'leur', 'amie', 'd’', 'enfance', ',', 'quatre', 'jeunes', 'femmes', 'à', 'l’', 'aube', 'de', 'la', 'trentaine', 'se', 'réunissent', 'dans', 'une', 'maison', 'de', 'campagne', '.']\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from six.moves import cPickle\n",
    "\n",
    "#load the model\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('./data/doc2vec.w2v')\n",
    "\n",
    "sentences_vector=[]\n",
    "\n",
    "t = 500\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i % t == 0:\n",
    "        print(\"sentence\", i, \":\", sentences[i])\n",
    "        print(\"***\")\n",
    "    sent = sentences[i]\n",
    "    sentences_vector.append(d2v_model.infer_vector(sent, alpha=0.001, min_alpha=0.001, steps=10000))\n",
    "    \n",
    "#save the sentences_vector\n",
    "sentences_vector_file = os.path.join(save_dir, \"sentences_vector_500_a001_ma001_s10000.pkl\")\n",
    "with open(os.path.join(sentences_vector_file), 'wb') as f:\n",
    "    cPickle.dump((sentences_vector), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sequence:  0\n",
      "   1 th vector for this sequence. Sentence  ID0 (vector dim =  500 )\n",
      "   2 th vector for this sequence. Sentence  ID1 (vector dim =  500 )\n",
      "   3 th vector for this sequence. Sentence  ID2 (vector dim =  500 )\n",
      "   4 th vector for this sequence. Sentence  ID3 (vector dim =  500 )\n",
      "   5 th vector for this sequence. Sentence  ID4 (vector dim =  500 )\n",
      "   6 th vector for this sequence. Sentence  ID5 (vector dim =  500 )\n",
      "   7 th vector for this sequence. Sentence  ID6 (vector dim =  500 )\n",
      "   8 th vector for this sequence. Sentence  ID7 (vector dim =  500 )\n",
      "   9 th vector for this sequence. Sentence  ID8 (vector dim =  500 )\n",
      "   10 th vector for this sequence. Sentence  ID9 (vector dim =  500 )\n",
      "   11 th vector for this sequence. Sentence  ID10 (vector dim =  500 )\n",
      "   12 th vector for this sequence. Sentence  ID11 (vector dim =  500 )\n",
      "   13 th vector for this sequence. Sentence  ID12 (vector dim =  500 )\n",
      "   14 th vector for this sequence. Sentence  ID13 (vector dim =  500 )\n",
      "   15 th vector for this sequence. Sentence  ID14 (vector dim =  500 )\n",
      "  y vector for this sequence  ID15 : (vector dim =  500 )\n",
      "(314, 15, 500) (314, 500)\n"
     ]
    }
   ],
   "source": [
    "nb_sequenced_sentences = 15\n",
    "vector_dim = 500\n",
    "\n",
    "X_train = np.zeros((len(sentences), nb_sequenced_sentences, vector_dim), dtype=np.float)\n",
    "y_train = np.zeros((len(sentences), vector_dim), dtype=np.float)\n",
    "\n",
    "t = 1000\n",
    "for i in range(len(sentences_label)-nb_sequenced_sentences-1):\n",
    "    if i % t == 0: print(\"new sequence: \", i)\n",
    "    \n",
    "    for k in range(nb_sequenced_sentences):\n",
    "        sent = sentences_label[i+k]\n",
    "        vect = sentences_vector[i+k]\n",
    "        \n",
    "        if i % t == 0:\n",
    "            print(\"  \", k + 1 ,\"th vector for this sequence. Sentence \", sent, \"(vector dim = \", len(vect), \")\")\n",
    "            \n",
    "        for j in range(len(vect)):\n",
    "            X_train[i, k, j] = vect[j]\n",
    "    \n",
    "    senty = sentences_label[i+nb_sequenced_sentences]\n",
    "    vecty = sentences_vector[i+nb_sequenced_sentences]\n",
    "    if i % t == 0: print(\"  y vector for this sequence \", senty, \": (vector dim = \", len(vecty), \")\")\n",
    "    for j in range(len(vecty)):\n",
    "        y_train[i, j] = vecty[j]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error, logcosh\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vector_dim):\n",
    "    print('Building LSTM model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vector_dim)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(vector_dim))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='logcosh', optimizer=optimizer, metrics=['acc'])\n",
    "    print('LSTM model built.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "LSTM model built.\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 512 # size of RNN\n",
    "vector_dim = 500\n",
    "learning_rate = 0.0001 #learning rate\n",
    "\n",
    "model_sequence = bidirectional_lstm_model(nb_sequenced_sentences, vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 282 samples, validate on 32 samples\n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.1063 - acc: 0.0000e+00 - val_loss: 0.0466 - val_acc: 0.0313\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.0928 - acc: 0.0142 - val_loss: 0.0435 - val_acc: 0.0313\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.0878 - acc: 0.0106 - val_loss: 0.0424 - val_acc: 0.0625\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.0849 - acc: 0.0142 - val_loss: 0.0419 - val_acc: 0.0625\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0834 - acc: 0.0284 - val_loss: 0.0415 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00005: saving model to save/my_model_sequence_lstm.05.hdf5\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0821 - acc: 0.0426 - val_loss: 0.0413 - val_acc: 0.0625\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.0810 - acc: 0.0319 - val_loss: 0.0412 - val_acc: 0.0625\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.0802 - acc: 0.0496 - val_loss: 0.0411 - val_acc: 0.0625\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0794 - acc: 0.0638 - val_loss: 0.0410 - val_acc: 0.0625\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0787 - acc: 0.0532 - val_loss: 0.0410 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00010: saving model to save/my_model_sequence_lstm.10.hdf5\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0781 - acc: 0.0390 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0776 - acc: 0.0567 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0770 - acc: 0.0461 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0765 - acc: 0.0496 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0758 - acc: 0.0532 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00015: saving model to save/my_model_sequence_lstm.15.hdf5\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0754 - acc: 0.0532 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0749 - acc: 0.0496 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0744 - acc: 0.0922 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0741 - acc: 0.0567 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0735 - acc: 0.0603 - val_loss: 0.0407 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00020: saving model to save/my_model_sequence_lstm.20.hdf5\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0730 - acc: 0.0887 - val_loss: 0.0407 - val_acc: 0.0625\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0726 - acc: 0.0674 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 23/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0722 - acc: 0.0851 - val_loss: 0.0407 - val_acc: 0.0625\n",
      "Epoch 24/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0718 - acc: 0.0887 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 25/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0715 - acc: 0.0745 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00025: saving model to save/my_model_sequence_lstm.25.hdf5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30 # minibatch size\n",
    "\n",
    "callbacks=[EarlyStopping(patience=5, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_sequence_lstm.{epoch:02d}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=5)]\n",
    "\n",
    "history = model_sequence.fit(X_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=40,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "#save the model\n",
    "model_sequence.save(save_dir + \"/\" + 'my_model_sequence_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
    {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'save' # directory to store models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy, and french model\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading doc2Vec model...\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "#load the doc2vec model\n",
    "print(\"loading doc2Vec model...\")\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('./data/doc2vec.w2v')\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n",
      "vocabulary loaded !\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(\"vocabulary loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word prediction model...\n",
      "model loaded!\n",
      "loading sentence selection model...\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the keras models\n",
    "print(\"loading word prediction model...\")\n",
    "model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')\n",
    "print(\"model loaded!\")\n",
    "print(\"loading sentence selection model...\")\n",
    "model_sequence = load_model(save_dir + \"/\" + 'my_model_sequence_lstm.final.hdf5')\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seed(seed_sentences,nb_words_in_seq=20, verbose=False):\n",
    "    #initiate sentences\n",
    "    generated = ''\n",
    "    sentence = []\n",
    "    \n",
    "    #fill the sentence with a default word\n",
    "    for i in range (nb_words_in_seq):\n",
    "        sentence.append(\"le\")\n",
    "\n",
    "    seed = seed_sentences.split()\n",
    "    \n",
    "    if verbose == True : print(\"seed: \",seed)\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[nb_words_in_seq-i-1]=seed[len(seed)-i-1]\n",
    "        #print(i, sentence)\n",
    "\n",
    "    generated += ' '.join(sentence)\n",
    "    \n",
    "    if verbose == True : print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
    "\n",
    "    return [generated, sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase(sentence, max_words = 50, nb_words_in_seq=20, temperature=1, verbose = False):\n",
    "    generated = \"\"\n",
    "    words_number = max_words - 1\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    seq_length = nb_words_in_seq\n",
    "    #sentence = []\n",
    "    is_punct = False\n",
    "    \n",
    "    #generate the text\n",
    "    for i in range(words_number):\n",
    "        #create the vector\n",
    "        x = np.zeros((1, seq_length, vocab_size))\n",
    "        for t, word in enumerate(sentence):\n",
    "            #print(t, word, vocab[word])\n",
    "            x[0, nb_words_in_seq-len(sentence)+t, vocab[word]] = 1.\n",
    "        #print(x.shape)\n",
    "\n",
    "        #calculate next word\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = vocabulary_inv[next_index]\n",
    "        \n",
    "        if verbose == True:\n",
    "            predv = np.array(preds)\n",
    "            #arr = np.array([1, 3, 2, 4, 5])\n",
    "            wi = predv.argsort()[-3:][::-1]\n",
    "            print(\"potential next words: \", vocabulary_inv[wi[0]], vocabulary_inv[wi[1]], vocabulary_inv[wi[2]])\n",
    "\n",
    "        #add the next word to the text\n",
    "        if is_punct == False:\n",
    "            if next_word in ponctuation:\n",
    "                is_punct = True\n",
    "            generated += \" \" + next_word\n",
    "            # shift the sentence by one, and and the next word at its end\n",
    "            sentence = sentence[1:] + [next_word]\n",
    "\n",
    "    return(generated, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_phrases_candidates(sentence, max_words = 50,\\\n",
    "                              nb_words_in_seq=20, \\\n",
    "                              temperature=1, \\\n",
    "                              nb_candidates_sents=10, \\\n",
    "                              verbose = False):\n",
    "    phrase_candidate = []\n",
    "    generated_sentence = \"\"\n",
    "    for i in range(nb_candidates_sents):\n",
    "        generated_sentence, new_sentence = generate_phrase(sentence, \\\n",
    "                                                           max_words = max_words, \\\n",
    "                                                           nb_words_in_seq = nb_words_in_seq, \\\n",
    "                                                           temperature=temperature, \\\n",
    "                                                           verbose = False)\n",
    "        phrase_candidate.append([generated_sentence, new_sentence])\n",
    "    \n",
    "    if verbose == True :\n",
    "        for phrase in phrase_candidate:\n",
    "            print(\"   \" , phrase[0])\n",
    "    return phrase_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_vector(sentences_list, verbose = False):\n",
    "    if verbose == True : print(\"generate vectors for each sentence...\")\n",
    "    seq = []\n",
    "    V = []\n",
    "\n",
    "    for s in sentences_list:\n",
    "        #infer the vector of the sentence, from the doc2vec model\n",
    "        v = d2v_model.infer_vector(create_sentences(nlp(s))[0], alpha=0.001, min_alpha=0.001, steps=10000)\n",
    "    #create the vector array for the model\n",
    "        V.append(v)\n",
    "    V_val=np.array(V)\n",
    "    #expand dimension to fit the entry of the model : that's the training vector\n",
    "    V_val = np.expand_dims(V_val, axis=0)\n",
    "    if verbose == True : print(\"Vectors generated!\")\n",
    "    return V_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_phrase(model, V_val, candidate_list, verbose=False):\n",
    "    sims_list = []\n",
    "    \n",
    "    #calculate prediction\n",
    "    preds = model.predict(V_val, verbose=0)[0]\n",
    "    \n",
    "    #calculate vector for each candidate\n",
    "    for candidate in candidate_list:\n",
    "        #calculate vector\n",
    "        #print(\"calculate vector for : \", candidate[1])\n",
    "        V = np.array(d2v_model.infer_vector(candidate[1]))\n",
    "        #calculate csonie similarity\n",
    "        sim = scipy.spatial.distance.cosine(V,preds)\n",
    "        #populate list of similarities\n",
    "        sims_list.append(sim)\n",
    "    \n",
    "    #select index of the biggest similarity\n",
    "    m = max(sims_list)\n",
    "    index_max = sims_list.index(m)\n",
    "    \n",
    "    if verbose == True :\n",
    "        print(\"selected phrase :\")\n",
    "        print(\"     \", candidate_list[index_max][0])\n",
    "    return candidate_list[index_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paragraphe(phrase_seed, sentences_seed, \\\n",
    "                        max_words = 50, \\\n",
    "                        nb_words_in_seq=20, \\\n",
    "                        temperature=1, \\\n",
    "                        nb_phrases=30, \\\n",
    "                        nb_candidates_sents=10, \\\n",
    "                        verbose=True):\n",
    "    \n",
    "    sentences_list = sentences_seed\n",
    "    sentence = phrase_seed   \n",
    "    text = []\n",
    "    \n",
    "    for p in range(nb_phrases):\n",
    "        if verbose == True : print(\"\")\n",
    "        if verbose == True : print(\"#############\")\n",
    "        print(\"phrase \",p+1, \"/\", nb_phrases)\n",
    "        if verbose == True : print(\"#############\")       \n",
    "        if verbose == True:\n",
    "            print('Sentence to generate phrase : ')\n",
    "            print(\"     \", sentence)\n",
    "            print(\"\")\n",
    "            print('List of sentences to constrain next phrase : ')\n",
    "            print(\"     \", sentences_list)\n",
    "            print(\"\")\n",
    "    \n",
    "        #generate seed training vector\n",
    "        V_val = generate_training_vector(sentences_list, verbose = verbose)\n",
    "\n",
    "        #generate phrase candidate\n",
    "        if verbose == True : print(\"generate phrases candidates...\")\n",
    "        phrases_candidates = define_phrases_candidates(sentence, \\\n",
    "                                                       max_words = max_words, \\\n",
    "                                                       nb_words_in_seq = nb_words_in_seq, \\\n",
    "                                                       temperature=temperature, \\\n",
    "                                                       nb_candidates_sents=nb_candidates_sents, \\\n",
    "                                                       verbose = verbose)\n",
    "        \n",
    "        if verbose == True : print(\"select next phrase...\")\n",
    "        next_phrase = select_next_phrase(model_sequence, \\\n",
    "                                         V_val,\n",
    "                                         phrases_candidates, \\\n",
    "                                         verbose=verbose)\n",
    "        \n",
    "        print(\"Next phrase: \",next_phrase[0])\n",
    "        if verbose == True :\n",
    "            print(\"\")\n",
    "            print(\"Shift phrases in sentences list...\")\n",
    "        for i in range(len(sentences_list)-1):\n",
    "            sentences_list[i]=sentences_list[i+1]\n",
    "\n",
    "        sentences_list[len(sentences_list)-1] = next_phrase[0]\n",
    "        \n",
    "        if verbose == True:\n",
    "            print(\"done.\")\n",
    "            print(\"new list of sentences :\")\n",
    "            print(\"     \", sentences_list)     \n",
    "        sentence = next_phrase[1]\n",
    "        \n",
    "        text.append(next_phrase[0])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Un mois après la mort de leur amie d’enfance, \"\n",
    "s2 = \"quatre jeunes femmes à l’aube de la trentaine se réunissent dans une maison de campagne.\"\n",
    "s3 = \"Elles posent sur la table le journal intime de la défunte .\"\n",
    "s4 = \"À travers les mots de Coco, ces filles aussi franches que différentes plongent dans leurs souvenirs : \"\n",
    "s5 = \"de la naissance de leur amitié à leur découverte de l’amour, de la sexualité et de la vie .\"\n",
    "s6 = \"Coco, c’est une rencontre avec une gang de filles .\"\n",
    "s7 = \"Une comédie dramatique qui se penche ouvertement sur nos différents rapports à l’amour : \"\n",
    "s8 = \"de la naïveté à la sexualité précoce, de l’abstinence aux aventures débridées, du romantisme à l’infidélité, des désirs aux désillusions en passant par l’homosexualité, la solitude, l’image corporelle et, surtout, le rêve de la maternité .\"\n",
    "s9 = \"Au fil des ans, sans trop s’en apercevoir, ces amies ont tissé entre elles la plus solide des relations amoureuses, \"\n",
    "s10 = \"celle qui survit au-delà de la mort .\"\n",
    "s11 = \"s' écrie le jeune homme .\"\n",
    "s12 = \"Premier texte de Nathalie Doummar, \"\n",
    "s13 = \"Coco faisait salle comble lors de sa création sur la scène de La Petite Licorne à l’hiver 2016 .\"\n",
    "s14 = \"Mathieu Quesnel, qui a précédemment signé la mise en scène de L’amour est un dumpling présenté dans le cadre des 5 à 7 de La Licorne, \"\n",
    "s15 = \"dirige ce quintette d’actrices .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Un mois après la mort de leur amie d’enfance, ', 'quatre jeunes femmes à l’aube de la trentaine se réunissent dans une maison de campagne.', 'Elles posent sur la table le journal intime de la défunte .', 'À travers les mots de Coco, ces filles aussi franches que différentes plongent dans leurs souvenirs\\xa0: ', 'de la naissance de leur amitié à leur découverte de l’amour, de la sexualité et de la vie .', 'Coco, c’est une rencontre avec une gang de filles .', 'Une comédie dramatique qui se penche ouvertement sur nos différents rapports à l’amour\\xa0: ', 'de la naïveté à la sexualité précoce, de l’abstinence aux aventures débridées, du romantisme à l’infidélité, des désirs aux désillusions en passant par l’homosexualité, la solitude, l’image corporelle et, surtout, le rêve de la maternité .', 'Au fil des ans, sans trop s’en apercevoir, ces amies ont tissé entre elles la plus solide des relations amoureuses, ', 'celle qui survit au-delà de la mort .', \"s' écrie le jeune homme .\", 'Premier texte de Nathalie Doummar, ', 'Coco faisait salle comble lors de sa création sur la scène de La Petite Licorne à l’hiver 2016 .', 'Mathieu Quesnel, qui a précédemment signé la mise en scène de L’amour est un dumpling présenté dans le cadre des 5 à 7 de La Licorne, ', 'dirige ce quintette d’actrices .']\n"
     ]
    }
   ],
   "source": [
    "sentences_list = [s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13,s14,s15]\n",
    "print(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L’amour est un dumpling présenté dans le cadre des 5 à 7 de La Licorne, dirige ce quintette d’actrices .\n",
      "['L’amour', 'est', 'un', 'dumpling', 'présenté', 'dans', 'le', 'cadre', 'des', '5', 'à', '7', 'de', 'La', 'Licorne,', 'dirige', 'ce', 'quintette', 'd’actrices', '.']\n"
     ]
    }
   ],
   "source": [
    "phrase_seed, sentences_seed = create_seed(s1 + \" \" + s2 + \" \" +\\\n",
    "                                          s3 + \" \" + s4+ \" \" + s5 + \" \" +\\\n",
    "                                          s6 + \" \" + s7 + \" \" + s8 + \" \" +\\\n",
    "                                          s9+ \" \" + s10 + \" \" + s11 + \" \" +\\\n",
    "                                          s12 + \" \" + s13 + \" \" + s14+ \" \" + s15,20)\n",
    "print(phrase_seed)\n",
    "print(sentences_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase  1 / 5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4f8a1b720213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_paragraphe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_list\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mnb_words_in_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m                           \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.201\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mnb_phrases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mnb_candidates_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-15a773a2b41d>\u001b[0m in \u001b[0;36mgenerate_paragraphe\u001b[0;34m(phrase_seed, sentences_seed, max_words, nb_words_in_seq, temperature, nb_phrases, nb_candidates_sents, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#generate seed training vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mV_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_training_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#generate phrase candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4922b9e8a3c5>\u001b[0m in \u001b[0;36mgenerate_training_vector\u001b[0;34m(sentences_list, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#infer the vector of the sentence, from the doc2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#create the vector array for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "text = generate_paragraphe(sentences_seed, sentences_list, \\\n",
    "                           max_words = 80, \\\n",
    "                           nb_words_in_seq = 30,\\\n",
    "                           temperature=0.201, \\\n",
    "                           nb_phrases=5, \\\n",
    "                           nb_candidates_sents=7, \\\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generated text: \")\n",
    "for t in text:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
