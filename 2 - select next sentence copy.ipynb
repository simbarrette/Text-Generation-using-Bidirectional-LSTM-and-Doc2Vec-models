{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "data_dir = 'data/News'# data directory containing input.txt\n",
    "save_dir = 'save' # directory to store models\n",
    "file_list = [\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import spacy, and french model\n",
    "import spacy\n",
    "nlp = spacy.load('fr')\n",
    "\n",
    "#initiate sentences and labels lists\n",
    "sentences = []\n",
    "sentences_label = []\n",
    "\n",
    "#create sentences function:\n",
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences\n",
    "\n",
    "#create sentences from files\n",
    "for file_name in file_list:\n",
    "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
    "    #read data\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    #create sentences\n",
    "    doc = nlp(data)\n",
    "    sents = create_sentences(doc)\n",
    "    sentences = sentences + sents\n",
    "    \n",
    "#create labels\n",
    "for i in range(np.array(sentences).shape[0]):\n",
    "    sentences_label.append(\"ID\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(data, docLabels, size=300, sample=0.000001, dm=0, hs=1, window=10, min_count=0, workers=8,alpha=0.024, min_alpha=0.024, epoch=15, save_file='./data/doc2vec.w2v') :\n",
    "    startime = time.time()\n",
    "    \n",
    "    print(\"{0} articles loaded for model\".format(len(data)))\n",
    "\n",
    "    it = LabeledLineSentence(data, docLabels)\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=size, sample=sample, dm=dm, window=window, min_count=min_count, workers=workers,alpha=alpha, min_alpha=min_alpha, hs=hs) # use fixed learning rate\n",
    "    model.build_vocab(it)\n",
    "    for epoch in range(epoch):\n",
    "        print(\"Training epoch {}\".format(epoch + 1))\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        # model.alpha -= 0.002 # decrease the learning rate\n",
    "        # model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "        \n",
    "    #saving the created model\n",
    "    model.save(os.path.join(save_file))\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314 articles loaded for model\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  import sys\n",
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(sentences, sentences_label, size=500,sample=0.0,alpha=0.025, min_alpha=0.001, min_count=0, window=10, epoch=20, dm=0, hs=1, save_file='./data/doc2vec.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : ['un', 'mois', 'après', 'la', 'mort', 'de', 'leur', 'amie', 'd’', 'enfance', ',', 'quatre', 'jeunes', 'femmes', 'à', 'l’', 'aube', 'de', 'la', 'trentaine', 'se', 'réunissent', 'dans', 'une', 'maison', 'de', 'campagne', '.']\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from six.moves import cPickle\n",
    "\n",
    "#load the model\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('./data/doc2vec.w2v')\n",
    "\n",
    "sentences_vector=[]\n",
    "\n",
    "t = 500\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i % t == 0:\n",
    "        print(\"sentence\", i, \":\", sentences[i])\n",
    "        print(\"***\")\n",
    "    sent = sentences[i]\n",
    "    sentences_vector.append(d2v_model.infer_vector(sent, alpha=0.001, min_alpha=0.001, steps=10000))\n",
    "    \n",
    "#save the sentences_vector\n",
    "sentences_vector_file = os.path.join(save_dir, \"sentences_vector_500_a001_ma001_s10000.pkl\")\n",
    "with open(os.path.join(sentences_vector_file), 'wb') as f:\n",
    "    cPickle.dump((sentences_vector), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sequence:  0\n",
      "   1 th vector for this sequence. Sentence  ID0 (vector dim =  500 )\n",
      "   2 th vector for this sequence. Sentence  ID1 (vector dim =  500 )\n",
      "   3 th vector for this sequence. Sentence  ID2 (vector dim =  500 )\n",
      "   4 th vector for this sequence. Sentence  ID3 (vector dim =  500 )\n",
      "   5 th vector for this sequence. Sentence  ID4 (vector dim =  500 )\n",
      "   6 th vector for this sequence. Sentence  ID5 (vector dim =  500 )\n",
      "   7 th vector for this sequence. Sentence  ID6 (vector dim =  500 )\n",
      "   8 th vector for this sequence. Sentence  ID7 (vector dim =  500 )\n",
      "   9 th vector for this sequence. Sentence  ID8 (vector dim =  500 )\n",
      "   10 th vector for this sequence. Sentence  ID9 (vector dim =  500 )\n",
      "   11 th vector for this sequence. Sentence  ID10 (vector dim =  500 )\n",
      "   12 th vector for this sequence. Sentence  ID11 (vector dim =  500 )\n",
      "   13 th vector for this sequence. Sentence  ID12 (vector dim =  500 )\n",
      "   14 th vector for this sequence. Sentence  ID13 (vector dim =  500 )\n",
      "   15 th vector for this sequence. Sentence  ID14 (vector dim =  500 )\n",
      "  y vector for this sequence  ID15 : (vector dim =  500 )\n",
      "(314, 15, 500) (314, 500)\n"
     ]
    }
   ],
   "source": [
    "nb_sequenced_sentences = 15\n",
    "vector_dim = 500\n",
    "\n",
    "X_train = np.zeros((len(sentences), nb_sequenced_sentences, vector_dim), dtype=np.float)\n",
    "y_train = np.zeros((len(sentences), vector_dim), dtype=np.float)\n",
    "\n",
    "t = 1000\n",
    "for i in range(len(sentences_label)-nb_sequenced_sentences-1):\n",
    "    if i % t == 0: print(\"new sequence: \", i)\n",
    "    \n",
    "    for k in range(nb_sequenced_sentences):\n",
    "        sent = sentences_label[i+k]\n",
    "        vect = sentences_vector[i+k]\n",
    "        \n",
    "        if i % t == 0:\n",
    "            print(\"  \", k + 1 ,\"th vector for this sequence. Sentence \", sent, \"(vector dim = \", len(vect), \")\")\n",
    "            \n",
    "        for j in range(len(vect)):\n",
    "            X_train[i, k, j] = vect[j]\n",
    "    \n",
    "    senty = sentences_label[i+nb_sequenced_sentences]\n",
    "    vecty = sentences_vector[i+nb_sequenced_sentences]\n",
    "    if i % t == 0: print(\"  y vector for this sequence \", senty, \": (vector dim = \", len(vecty), \")\")\n",
    "    for j in range(len(vecty)):\n",
    "        y_train[i, j] = vecty[j]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error, logcosh\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vector_dim):\n",
    "    print('Building LSTM model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vector_dim)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(vector_dim))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='logcosh', optimizer=optimizer, metrics=['acc'])\n",
    "    print('LSTM model built.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "LSTM model built.\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 512 # size of RNN\n",
    "vector_dim = 500\n",
    "learning_rate = 0.0001 #learning rate\n",
    "\n",
    "model_sequence = bidirectional_lstm_model(nb_sequenced_sentences, vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 282 samples, validate on 32 samples\n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.1063 - acc: 0.0000e+00 - val_loss: 0.0466 - val_acc: 0.0313\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.0928 - acc: 0.0142 - val_loss: 0.0435 - val_acc: 0.0313\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.0878 - acc: 0.0106 - val_loss: 0.0424 - val_acc: 0.0625\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 3s 9ms/step - loss: 0.0849 - acc: 0.0142 - val_loss: 0.0419 - val_acc: 0.0625\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0834 - acc: 0.0284 - val_loss: 0.0415 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00005: saving model to save/my_model_sequence_lstm.05.hdf5\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0821 - acc: 0.0426 - val_loss: 0.0413 - val_acc: 0.0625\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.0810 - acc: 0.0319 - val_loss: 0.0412 - val_acc: 0.0625\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 3s 11ms/step - loss: 0.0802 - acc: 0.0496 - val_loss: 0.0411 - val_acc: 0.0625\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0794 - acc: 0.0638 - val_loss: 0.0410 - val_acc: 0.0625\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0787 - acc: 0.0532 - val_loss: 0.0410 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00010: saving model to save/my_model_sequence_lstm.10.hdf5\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0781 - acc: 0.0390 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0776 - acc: 0.0567 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0770 - acc: 0.0461 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0765 - acc: 0.0496 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0758 - acc: 0.0532 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00015: saving model to save/my_model_sequence_lstm.15.hdf5\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0754 - acc: 0.0532 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0749 - acc: 0.0496 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0744 - acc: 0.0922 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0741 - acc: 0.0567 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0735 - acc: 0.0603 - val_loss: 0.0407 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00020: saving model to save/my_model_sequence_lstm.20.hdf5\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0730 - acc: 0.0887 - val_loss: 0.0407 - val_acc: 0.0625\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0726 - acc: 0.0674 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 23/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0722 - acc: 0.0851 - val_loss: 0.0407 - val_acc: 0.0625\n",
      "Epoch 24/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0718 - acc: 0.0887 - val_loss: 0.0408 - val_acc: 0.0625\n",
      "Epoch 25/40\n",
      "282/282 [==============================] - 3s 10ms/step - loss: 0.0715 - acc: 0.0745 - val_loss: 0.0409 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00025: saving model to save/my_model_sequence_lstm.25.hdf5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30 # minibatch size\n",
    "\n",
    "callbacks=[EarlyStopping(patience=5, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_sequence_lstm.{epoch:02d}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=5)]\n",
    "\n",
    "history = model_sequence.fit(X_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=40,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "#save the model\n",
    "model_sequence.save(save_dir + \"/\" + 'my_model_sequence_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
