{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "It is now time to generate some text from the models we trained!\n",
    "\n",
    "As a recap:\n",
    " - we trained a **first bidirectional LSTM model** to predict the next word of a given sequence of 30 words.\n",
    " - we train a **doc2vec model** for the whole input text as space, sentence based,\n",
    " - we trained a **second bidirectional LSTM model** to predict the best vectorized-sentence, following a sequence of 15 vectorized-phrases.\n",
    " \n",
    "So, what will be the process of our text generation ? We have first to provide a seed of 15 sentences, that contain at least 30 words. then:\n",
    " 1. using the last 30 words of the seed, we generate 10 candidates sentences.\n",
    " 2. we infer their vectors using the doc2vec model,\n",
    " 3. we calculate the \"best vector\" for the sentence following the 15 phrases of the seed,\n",
    " 4. we compare the infered vectors with the \"best vector\", and pick-up the closest one.\n",
    " 5. we add the generated sentence corresponding to this vector at the end of the seed, as the next sentence of the text.\n",
    " 6. then, we loop over the process.\n",
    " \n",
    "## 0. import libraries and parameters\n",
    "\n",
    "In order to start, we have to import our models and retrieve our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'save' # directory to store models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy, and french model\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading doc2Vec model...\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "#load the doc2vec model\n",
    "print(\"loading doc2Vec model...\")\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('./data/doc2vec.w2v')\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n",
      "vocabulary loaded !\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(\"vocabulary loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbarrette/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word prediction model...\n",
      "model loaded!\n",
      "loading sentence selection model...\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the keras models\n",
    "print(\"loading word prediction model...\")\n",
    "model = load_model(save_dir + \"/\" + 'my_model_gen_sentences_lstm.final.hdf5')\n",
    "print(\"model loaded!\")\n",
    "print(\"loading sentence selection model...\")\n",
    "model_sequence = load_model(save_dir + \"/\" + 'my_model_sequence_lstm.final.hdf5')\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Functions to generate Candidates Sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the text generation, and tune a bit the word prediction, we introduce a specific function to pick-up words from our vocabulary.\n",
    "\n",
    "We will not take the words with the highest prediction (or the generation of text will be boring), but would like to insert some uncertainties, and let the solution, sometime, to pick-up words with less good prediction.\n",
    "\n",
    "That is the purpose of the function **sample()**, that will draw randomly a word from our vocabulary.\n",
    "\n",
    "However, the probability for a word to be drawn will depends directly on its probability to be the next word, thanks to our first bidirectional LSTM Model.\n",
    "\n",
    "In order to tune this probability, we introduce a \"temperature\" to smooth or sharpen its value.\n",
    " - **if _temperature = 1.0_**, the probability for a word to be drawn is equal to the probability for the word to be the next one in the sequence (output of the owrd prediction model),\n",
    " - **if _temperature_ is big (much bigger than 1)**, the range of probabilities is shorten: the probabilities for all words to be the next one is closer to 1. More variety of words will be picked-up from the vocabulary.\n",
    " - **if _temperatune_ is small (close to 0)**, small probabilities will be avoided (they will be set closed to 0). Less words will be picked-up from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **create_seed()** function is usefull to prepare seed sequences, especially if the number of words in the seed phrase is lower than the espected number for a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seed(seed_sentences,nb_words_in_seq=20, verbose=False):\n",
    "    #initiate sentences\n",
    "    generated = ''\n",
    "    sentence = []\n",
    "    \n",
    "    #fill the sentence with a default word\n",
    "    for i in range (nb_words_in_seq):\n",
    "        sentence.append(\"le\")\n",
    "\n",
    "    seed = seed_sentences.split()\n",
    "    \n",
    "    if verbose == True : print(\"seed: \",seed)\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[nb_words_in_seq-i-1]=seed[len(seed)-i-1]\n",
    "        #print(i, sentence)\n",
    "\n",
    "    generated += ' '.join(sentence)\n",
    "    \n",
    "    if verbose == True : print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
    "\n",
    "    return [generated, sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function **generate_phrase()** is used to create the next phrase of a given sentence.\n",
    "\n",
    "It requires as inputs:\n",
    " - the previous sentence,\n",
    " - the maximum number of words in the phrase,\n",
    " - the temperature of the sample function.\n",
    " \n",
    "If a punctuation word is reached before the maximum number of the words, the function ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase(sentence, max_words = 50, nb_words_in_seq=20, temperature=1, verbose = False):\n",
    "    generated = \"\"\n",
    "    words_number = max_words - 1\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"â€¦\"]\n",
    "    seq_length = nb_words_in_seq\n",
    "    #sentence = []\n",
    "    is_punct = False\n",
    "    \n",
    "    #generate the text\n",
    "    for i in range(words_number):\n",
    "        #create the vector\n",
    "        x = np.zeros((1, seq_length, vocab_size))\n",
    "        for t, word in enumerate(sentence):\n",
    "            #print(t, word, vocab[word])\n",
    "            x[0, nb_words_in_seq-len(sentence)+t, vocab[word]] = 1.\n",
    "        #print(x.shape)\n",
    "\n",
    "        #calculate next word\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = vocabulary_inv[next_index]\n",
    "        \n",
    "        if verbose == True:\n",
    "            predv = np.array(preds)\n",
    "            #arr = np.array([1, 3, 2, 4, 5])\n",
    "            wi = predv.argsort()[-3:][::-1]\n",
    "            print(\"potential next words: \", vocabulary_inv[wi[0]], vocabulary_inv[wi[1]], vocabulary_inv[wi[2]])\n",
    "\n",
    "        #add the next word to the text\n",
    "        if is_punct == False:\n",
    "            if next_word in ponctuation:\n",
    "                is_punct = True\n",
    "            generated += \" \" + next_word\n",
    "            # shift the sentence by one, and and the next word at its end\n",
    "            sentence = sentence[1:] + [next_word]\n",
    "\n",
    "    return(generated, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function **define_phrases_candidates()** provides a list of potential phrases, for a given previous sentence and a specific temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_phrases_candidates(sentence, max_words = 50,\\\n",
    "                              nb_words_in_seq=20, \\\n",
    "                              temperature=1, \\\n",
    "                              nb_candidates_sents=10, \\\n",
    "                              verbose = False):\n",
    "    phrase_candidate = []\n",
    "    generated_sentence = \"\"\n",
    "    for i in range(nb_candidates_sents):\n",
    "        generated_sentence, new_sentence = generate_phrase(sentence, \\\n",
    "                                                           max_words = max_words, \\\n",
    "                                                           nb_words_in_seq = nb_words_in_seq, \\\n",
    "                                                           temperature=temperature, \\\n",
    "                                                           verbose = False)\n",
    "        phrase_candidate.append([generated_sentence, new_sentence])\n",
    "    \n",
    "    if verbose == True :\n",
    "        for phrase in phrase_candidate:\n",
    "            print(\"   \" , phrase[0])\n",
    "    return phrase_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions to select the best sentence\n",
    "\n",
    "the **create_sentences()** function generate a sequence of words (a list) for a given spacy doc item.\n",
    "\n",
    "It will be used to create a sequence of words from a single phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"â€¦\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the **generate_training_vector()** function is used to predict the next vectorized-sentence for a given sequence of vectorized-sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_vector(sentences_list, verbose = False):\n",
    "    if verbose == True : print(\"generate vectors for each sentence...\")\n",
    "    seq = []\n",
    "    V = []\n",
    "\n",
    "    for s in sentences_list:\n",
    "        #infer the vector of the sentence, from the doc2vec model\n",
    "        v = d2v_model.infer_vector(create_sentences(nlp(s))[0], alpha=0.001, min_alpha=0.001, steps=10000)\n",
    "    #create the vector array for the model\n",
    "        V.append(v)\n",
    "    V_val=np.array(V)\n",
    "    #expand dimension to fit the entry of the model : that's the training vector\n",
    "    V_val = np.expand_dims(V_val, axis=0)\n",
    "    if verbose == True : print(\"Vectors generated!\")\n",
    "    return V_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **select_next_phrase()** function allows us to pick-up the best candidates for the next phrase.\n",
    "\n",
    "First, it calculates the vector for each candidates.\n",
    "\n",
    "Then, based on the vector generated by the function **generate_training_vector()**, it performs a cosine similarity with them and pick the one with the biggest similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_phrase(model, V_val, candidate_list, verbose=False):\n",
    "    sims_list = []\n",
    "    \n",
    "    #calculate prediction\n",
    "    preds = model.predict(V_val, verbose=0)[0]\n",
    "    \n",
    "    #calculate vector for each candidate\n",
    "    for candidate in candidate_list:\n",
    "        #calculate vector\n",
    "        #print(\"calculate vector for : \", candidate[1])\n",
    "        V = np.array(d2v_model.infer_vector(candidate[1]))\n",
    "        #calculate csonie similarity\n",
    "        sim = scipy.spatial.distance.cosine(V,preds)\n",
    "        #populate list of similarities\n",
    "        sims_list.append(sim)\n",
    "    \n",
    "    #select index of the biggest similarity\n",
    "    m = max(sims_list)\n",
    "    index_max = sims_list.index(m)\n",
    "    \n",
    "    if verbose == True :\n",
    "        print(\"selected phrase :\")\n",
    "        print(\"     \", candidate_list[index_max][0])\n",
    "    return candidate_list[index_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text generation - workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, **generate_paragraph()**, combines all previous functions to generate the text.\n",
    "\n",
    "With the following parameters:\n",
    " - phrase_seed : the sentence seed for the first word prediction. It is a list of words.\n",
    " - sentences_seed : the seed sequence of sentences. It is a list of sentences.\n",
    " - max_words: the maximum number of words for a new generated sentence.\n",
    " - nb_words_in_seq: the number of words to keep as seed for the next word prediction.\n",
    " - temperature: the temperature for the word prediction.\n",
    " - nb_phrases: the number of phrase (sentence) to generate.\n",
    " - nb_candidates_sents: the number of phrase candidates to generate for each new phrase.\n",
    " - verbose: verbosity of the script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paragraphe(phrase_seed, sentences_seed, \\\n",
    "                        max_words = 50, \\\n",
    "                        nb_words_in_seq=20, \\\n",
    "                        temperature=1, \\\n",
    "                        nb_phrases=30, \\\n",
    "                        nb_candidates_sents=10, \\\n",
    "                        verbose=True):\n",
    "    \n",
    "    sentences_list = sentences_seed\n",
    "    sentence = phrase_seed   \n",
    "    text = []\n",
    "    \n",
    "    for p in range(nb_phrases):\n",
    "        if verbose == True : print(\"\")\n",
    "        if verbose == True : print(\"#############\")\n",
    "        print(\"phrase \",p+1, \"/\", nb_phrases)\n",
    "        if verbose == True : print(\"#############\")       \n",
    "        if verbose == True:\n",
    "            print('Sentence to generate phrase : ')\n",
    "            print(\"     \", sentence)\n",
    "            print(\"\")\n",
    "            print('List of sentences to constrain next phrase : ')\n",
    "            print(\"     \", sentences_list)\n",
    "            print(\"\")\n",
    "    \n",
    "        #generate seed training vector\n",
    "        V_val = generate_training_vector(sentences_list, verbose = verbose)\n",
    "\n",
    "        #generate phrase candidate\n",
    "        if verbose == True : print(\"generate phrases candidates...\")\n",
    "        phrases_candidates = define_phrases_candidates(sentence, \\\n",
    "                                                       max_words = max_words, \\\n",
    "                                                       nb_words_in_seq = nb_words_in_seq, \\\n",
    "                                                       temperature=temperature, \\\n",
    "                                                       nb_candidates_sents=nb_candidates_sents, \\\n",
    "                                                       verbose = verbose)\n",
    "        \n",
    "        if verbose == True : print(\"select next phrase...\")\n",
    "        next_phrase = select_next_phrase(model_sequence, \\\n",
    "                                         V_val,\n",
    "                                         phrases_candidates, \\\n",
    "                                         verbose=verbose)\n",
    "        \n",
    "        print(\"Next phrase: \",next_phrase[0])\n",
    "        if verbose == True :\n",
    "            print(\"\")\n",
    "            print(\"Shift phrases in sentences list...\")\n",
    "        for i in range(len(sentences_list)-1):\n",
    "            sentences_list[i]=sentences_list[i+1]\n",
    "\n",
    "        sentences_list[len(sentences_list)-1] = next_phrase[0]\n",
    "        \n",
    "        if verbose == True:\n",
    "            print(\"done.\")\n",
    "            print(\"new list of sentences :\")\n",
    "            print(\"     \", sentences_list)     \n",
    "        sentence = next_phrase[1]\n",
    "        \n",
    "        text.append(next_phrase[0])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can perform the complete text generation workflow.\n",
    "\n",
    "First, we have to define the sentences in the seed (15 phrases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Un mois aprÃ¨s la mort de leur amie dâ€™enfance, \"\n",
    "s2 = \"quatre jeunes femmes Ã  lâ€™aube de la trentaine se rÃ©unissent dans une maison de campagne.\"\n",
    "s3 = \"Elles posent sur la table le journal intime de la dÃ©funte .\"\n",
    "s4 = \"Ã€ travers les mots de Coco, ces filles aussi franches que diffÃ©rentes plongent dans leurs souvenirsÂ : \"\n",
    "s5 = \"de la naissance de leur amitiÃ© Ã  leur dÃ©couverte de lâ€™amour, de la sexualitÃ© et de la vie .\"\n",
    "s6 = \"Coco, câ€™est une rencontre avec une gang de filles .\"\n",
    "s7 = \"Une comÃ©die dramatique qui se penche ouvertement sur nos diffÃ©rents rapports Ã  lâ€™amourÂ : \"\n",
    "s8 = \"de la naÃ¯vetÃ© Ã  la sexualitÃ© prÃ©coce, de lâ€™abstinence aux aventures dÃ©bridÃ©es, du romantisme Ã  lâ€™infidÃ©litÃ©, des dÃ©sirs aux dÃ©sillusions en passant par lâ€™homosexualitÃ©, la solitude, lâ€™image corporelle et, surtout, le rÃªve de la maternitÃ© .\"\n",
    "s9 = \"Au fil des ans, sans trop sâ€™en apercevoir, ces amies ont tissÃ© entre elles la plus solide des relations amoureuses, \"\n",
    "s10 = \"celle qui survit au-delÃ  de la mort .\"\n",
    "s11 = \"s' Ã©crie le jeune homme .\"\n",
    "s12 = \"Premier texte de Nathalie Doummar, \"\n",
    "s13 = \"Coco faisait salle comble lors de sa crÃ©ation sur la scÃ¨ne de La Petite Licorne Ã  lâ€™hiver 2016 .\"\n",
    "s14 = \"Mathieu Quesnel, qui a prÃ©cÃ©demment signÃ© la mise en scÃ¨ne de Lâ€™amour est un dumpling prÃ©sentÃ© dans le cadre des 5 Ã  7 de La Licorne, \"\n",
    "s15 = \"dirige ce quintette dâ€™actrices .\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine them in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Un mois aprÃ¨s la mort de leur amie dâ€™enfance, ', 'quatre jeunes femmes Ã  lâ€™aube de la trentaine se rÃ©unissent dans une maison de campagne.', 'Elles posent sur la table le journal intime de la dÃ©funte .', 'Ã€ travers les mots de Coco, ces filles aussi franches que diffÃ©rentes plongent dans leurs souvenirs\\xa0: ', 'de la naissance de leur amitiÃ© Ã  leur dÃ©couverte de lâ€™amour, de la sexualitÃ© et de la vie .', 'Coco, câ€™est une rencontre avec une gang de filles .', 'Une comÃ©die dramatique qui se penche ouvertement sur nos diffÃ©rents rapports Ã  lâ€™amour\\xa0: ', 'de la naÃ¯vetÃ© Ã  la sexualitÃ© prÃ©coce, de lâ€™abstinence aux aventures dÃ©bridÃ©es, du romantisme Ã  lâ€™infidÃ©litÃ©, des dÃ©sirs aux dÃ©sillusions en passant par lâ€™homosexualitÃ©, la solitude, lâ€™image corporelle et, surtout, le rÃªve de la maternitÃ© .', 'Au fil des ans, sans trop sâ€™en apercevoir, ces amies ont tissÃ© entre elles la plus solide des relations amoureuses, ', 'celle qui survit au-delÃ  de la mort .', \"s' Ã©crie le jeune homme .\", 'Premier texte de Nathalie Doummar, ', 'Coco faisait salle comble lors de sa crÃ©ation sur la scÃ¨ne de La Petite Licorne Ã  lâ€™hiver 2016 .', 'Mathieu Quesnel, qui a prÃ©cÃ©demment signÃ© la mise en scÃ¨ne de Lâ€™amour est un dumpling prÃ©sentÃ© dans le cadre des 5 Ã  7 de La Licorne, ', 'dirige ce quintette dâ€™actrices .']\n"
     ]
    }
   ],
   "source": [
    "sentences_list = [s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13,s14,s15]\n",
    "print(sentences_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate them in a single phrase and create the seed sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lâ€™amour est un dumpling prÃ©sentÃ© dans le cadre des 5 Ã  7 de La Licorne, dirige ce quintette dâ€™actrices .\n",
      "['Lâ€™amour', 'est', 'un', 'dumpling', 'prÃ©sentÃ©', 'dans', 'le', 'cadre', 'des', '5', 'Ã ', '7', 'de', 'La', 'Licorne,', 'dirige', 'ce', 'quintette', 'dâ€™actrices', '.']\n"
     ]
    }
   ],
   "source": [
    "phrase_seed, sentences_seed = create_seed(s1 + \" \" + s2 + \" \" +\\\n",
    "                                          s3 + \" \" + s4+ \" \" + s5 + \" \" +\\\n",
    "                                          s6 + \" \" + s7 + \" \" + s8 + \" \" +\\\n",
    "                                          s9+ \" \" + s10 + \" \" + s11 + \" \" +\\\n",
    "                                          s12 + \" \" + s13 + \" \" + s14+ \" \" + s15,20)\n",
    "print(phrase_seed)\n",
    "print(sentences_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script to generate the text !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase  1 / 5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4f8a1b720213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_paragraphe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_list\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mnb_words_in_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m                           \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.201\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mnb_phrases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mnb_candidates_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-15a773a2b41d>\u001b[0m in \u001b[0;36mgenerate_paragraphe\u001b[0;34m(phrase_seed, sentences_seed, max_words, nb_words_in_seq, temperature, nb_phrases, nb_candidates_sents, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#generate seed training vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mV_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_training_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#generate phrase candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4922b9e8a3c5>\u001b[0m in \u001b[0;36mgenerate_training_vector\u001b[0;34m(sentences_list, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#infer the vector of the sentence, from the doc2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#create the vector array for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "text = generate_paragraphe(sentences_seed, sentences_list, \\\n",
    "                           max_words = 80, \\\n",
    "                           nb_words_in_seq = 30,\\\n",
    "                           temperature=0.201, \\\n",
    "                           nb_phrases=5, \\\n",
    "                           nb_candidates_sents=7, \\\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the new text generated is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generated text: \")\n",
    "for t in text:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
